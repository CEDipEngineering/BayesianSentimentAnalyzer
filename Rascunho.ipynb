{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Ci√™ncia dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Carlos Eduardo Dip\n",
    "\n",
    "Nome: Gianluca Lazzaris Giudici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ser√£o permitidos grupos de tr√™s pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisar√£o fazer um question√°rio de avalia√ß√£o de trabalho em equipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Classificador autom√°tico de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente no jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import functools\n",
    "import operator\n",
    "from random import shuffle\n",
    "\n",
    "try:\n",
    "    import emoji\n",
    "except:\n",
    "    !pip install emoji --upgrade\n",
    "    import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Coleta de Dados:\n",
    "\n",
    "Os dados foram coletados em outro notebook, chamado Coletor.ipynb. Abaixo, ser√° importado o excel com os dados j√° classificados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Limpeza, aquisi√ß√£o e organiza√ß√£o dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training = pd.read_excel('data/Uber.xlsx')\n",
    "data_validation = pd.read_excel('data/Uber.xlsx', 'Teste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training_R = data_training.loc[data_training.Classificacao == 1]\n",
    "data_training_NR = data_training.loc[data_training.Classificacao == 0]\n",
    "\n",
    "TrainingString_Relevant = ''\n",
    "TrainingString_NotRelevant = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_laughter(word):\n",
    "    for letter in word:\n",
    "        if letter != 'k':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def cleanup(text):\n",
    "\n",
    "    text_split_emoji = emoji.get_emoji_regexp().split(text)\n",
    "    text_split_whitespace = [substr.split() for substr in text_split_emoji]\n",
    "    text_split = functools.reduce(operator.concat, text_split_whitespace)\n",
    "    text_split = ' '.join(word for word in text_split)\n",
    "    \n",
    "    text = ' '.join(word for word in text_split.split() if not word.startswith('https'))\n",
    "    \n",
    "    \n",
    "    punctuation = '[!-.:?;]'\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    text_subbed = text_subbed.lower()\n",
    "    text_out = ' '\n",
    "    \n",
    "    \n",
    "    for word in text_subbed.split():\n",
    "        if check_laughter(word):\n",
    "            text_out += ' haha'\n",
    "        else:\n",
    "            text_out += ' ' + word\n",
    "    \n",
    "    \n",
    "    return text_out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  x gon give it to ya fuck wait for you to get it on your own x gon deliver to ya knock knock open up the door it s real wit the non stop pop pop and stainless steel go hard gettin busy wit it uber blah blah haha üë®üèø üò∑ üò∑ uber blah blah haha üë®üèø üò∑ üò∑\n",
      "['x', 'gon', 'give', 'it', 'to', 'ya', 'fuck', 'wait', 'for', 'you', 'to', 'get', 'it', 'on', 'your', 'own', 'x', 'gon', 'deliver', 'to', 'ya', 'knock', 'knock', 'open', 'up', 'the', 'door', 'it', 's', 'real', 'wit', 'the', 'non', 'stop', 'pop', 'pop', 'and', 'stainless', 'steel', 'go', 'hard', 'gettin', 'busy', 'wit', 'it', 'uber', 'blah', 'blah', 'haha', 'üë®üèø', 'üò∑', 'üò∑', 'uber', 'blah', 'blah', 'haha', 'üë®üèø', 'üò∑', 'üò∑']\n"
     ]
    }
   ],
   "source": [
    "### --- Cleanup function demo --- ###\n",
    "demo_string_ =  \"\"\"\n",
    "                X gon give it to ya\n",
    "                Fuck wait for you to get it on your own\n",
    "                X gon deliver to ya\n",
    "                Knock knock, open up the door, it's real\n",
    "                Wit the non-stop, pop pop and stainless steel\n",
    "                Go hard gettin busy wit it\n",
    "                https://img.imgur/1337, uber blah blah kkk, üë®üèøüò∑üò∑\n",
    "                https://img.imgur/1337, uber blah blah kkk, üë®üèøüò∑üò∑\n",
    "                \"\"\"\n",
    "\n",
    "print(cleanup(demo_string_))\n",
    "print(cleanup(demo_string_).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phrase in data_training_R.Treinamento:\n",
    "    TrainingString_Relevant += phrase\n",
    "for phrase in data_training_NR.Treinamento:\n",
    "    TrainingString_NotRelevant += phrase    \n",
    "    \n",
    "cleanup(TrainingString_NotRelevant)\n",
    "cleanup(TrainingString_Relevant)\n",
    "\n",
    "TrainingSeries_Relevant = pd.Series(TrainingString_Relevant.split()).value_counts(True)\n",
    "TrainingSeries_NotRelevant = pd.Series(TrainingString_NotRelevant.split()).value_counts(True)\n",
    "\n",
    "for word in ['o', 'de', 'e', 'que', 'do', 'a', 'pra', 'um']:\n",
    "    del TrainingSeries_NotRelevant[word]\n",
    "    del TrainingSeries_Relevant[word]\n",
    "\n",
    "LaplaceConstant = 0.0000001\n",
    "\n",
    "### --- Laplace Smoothing --- ###\n",
    "TrainingSeries_Relevant += LaplaceConstant\n",
    "TrainingSeries_NotRelevant += LaplaceConstant\n",
    "\n",
    "### --- Log of everything to prevent underflow --- ###\n",
    "TrainingSeries_Relevant = np.log(TrainingSeries_Relevant)\n",
    "TrainingSeries_NotRelevant = np.log(TrainingSeries_NotRelevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando o Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47374301675977654 0.5262569832402234\n"
     ]
    }
   ],
   "source": [
    "P_Relevant = len(TrainingSeries_Relevant)/(len(TrainingSeries_Relevant)+len(TrainingSeries_NotRelevant))\n",
    "P_NotRelevant = len(TrainingSeries_NotRelevant)/(len(TrainingSeries_Relevant)+len(TrainingSeries_NotRelevant))\n",
    "print(P_NotRelevant, P_Relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_relevance(tweet = \"\"):\n",
    "    \n",
    "    ## -- Cleaning tweet\n",
    "    text = cleanup(tweet)\n",
    "    \n",
    "#     ## -- Evaluates likelihood\n",
    "    sumR = 0\n",
    "    sumNR = 0\n",
    "    \n",
    "    for word in text.split():\n",
    "    \n",
    "        if word in TrainingSeries_Relevant: \n",
    "            sumR += TrainingSeries_Relevant[word]\n",
    "        else:\n",
    "            sumR += np.log(LaplaceConstant)\n",
    "        if word in TrainingSeries_NotRelevant:\n",
    "            sumNR += TrainingSeries_NotRelevant[word]\n",
    "        else:\n",
    "            sumNR += np.log(LaplaceConstant)\n",
    "    \n",
    "    sumR += np.log(P_Relevant)\n",
    "    sumNR += np.log(P_NotRelevant)\n",
    "    \n",
    "    return sumR>sumNR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance\n",
    "\n",
    "Agora voc√™ deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = [int(evaluate_relevance(tweet)) for tweet in data_validation.Teste]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_validation['bayes'] = evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'True-True': 0.2914572864321608,\n",
       " 'True-False': 0.23115577889447236,\n",
       " 'False-True': 0.24623115577889448,\n",
       " 'False-False': 0.23115577889447236,\n",
       " 'Final-Accuracy': '52.261%'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy = {'True-True':0,\n",
    "            'True-False':0,\n",
    "            'False-True':0,\n",
    "            'False-False':0\n",
    "           }\n",
    "\n",
    "\n",
    "for tweet,evaluation in zip(data_validation.classificacao, data_validation.bayes):\n",
    "    if tweet and evaluation:\n",
    "        Accuracy['True-True'] += 1\n",
    "    elif tweet and not evaluation:\n",
    "        Accuracy['True-False'] += 1\n",
    "    elif not tweet and evaluation:\n",
    "        Accuracy['False-True'] += 1\n",
    "    elif not tweet and not evaluation:\n",
    "        Accuracy['False-False'] += 1\n",
    "\n",
    "Accuracy_Normalized = {}\n",
    "S = sum(Accuracy.values())\n",
    "for k,v in Accuracy.items():\n",
    "    Accuracy_Normalized[k] = v/S\n",
    "Accuracy_Normalized['Final-Accuracy'] = str(round((Accuracy_Normalized['True-True']+Accuracy_Normalized['False-False'])*100, 3)) + '%'\n",
    "Accuracy_Normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperfei√ßoamento:\n",
    "\n",
    "Os trabalhos v√£o evoluir em conceito dependendo da quantidade de itens avan√ßados:\n",
    "\n",
    "* Limpar: \\n, :, \", ', (, ), etc SEM remover emojis\n",
    "* Corrigir separa√ß√£o de espa√ßos entre palavras e emojis ou emojis e emojis\n",
    "* Propor outras limpezas e transforma√ß√µes que n√£o afetem a qualidade da informa√ß√£o ou classifica√ß√£o\n",
    "* Criar categorias intermedi√°rias de relev√¢ncia baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante (3 categorias: C, mais categorias conta para B)\n",
    "* Explicar por que n√£o posso usar o pr√≥prio classificador para gerar mais amostras de treinamento\n",
    "* Propor diferentes cen√°rios para Na√Øve Bayes fora do contexto do projeto\n",
    "* Sugerir e explicar melhorias reais com indica√ß√µes concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* Montar um dashboard que periodicamente realiza an√°lise de sentimento e visualiza estes dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refer√™ncias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**\n",
    "\n",
    "[Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) **Explica√ß√£o da t√©cnica para lidar com palavras novas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
