{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Ci√™ncia dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Carlos Eduardo Dip\n",
    "\n",
    "Nome: Gianluca Lazzaris Giudici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposta\n",
    "___\n",
    "\n",
    "Neste projeto, a proposta √© criar um identificador de sentimento, a partir de um analisador Na√Øve-Bayes, que faz uso principalmente do Teorema de Bayes, e √© um modelo de Machine Learning.\n",
    "___\n",
    "# Classificador autom√°tico de sentimento\n",
    "\n",
    "<img src = 'logo.jpg' style = 'width:30%'>\n",
    "\n",
    "Empresa escolhida: Uber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Preparando o ambiente no jupyter:\n",
    "\n",
    "Primeiro importamos os m√≥dulos que ser√£o utilizados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import functools\n",
    "import operator\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import emoji\n",
    "except:\n",
    "    !pip install emoji --upgrade\n",
    "    import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Coleta de Dados:\n",
    "\n",
    "Para a an√°lise que ser√° feita, ser√£o usados tweets (da plataforma Twitter), que ser√£o extra√≠dos atrav√©s de uma API chamada Tweepy;\n",
    "Os dados foram coletados em outro notebook, chamado Coletor.ipynb. Abaixo, ser√° importado o excel com os dados j√° classificados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Limpeza, aquisi√ß√£o e organiza√ß√£o dos dados\n",
    "\n",
    "Primeiro, constru√≠mos dataframes (da biblioteca Pandas), para facilitar a visualiza√ß√£o e utiliza√ß√£o dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training = pd.read_excel('data/Uber.xlsx')\n",
    "data_validation = pd.read_excel('data/Uber.xlsx', 'Teste')\n",
    "\n",
    "data_training_R = data_training.loc[data_training.Classificacao == 1]\n",
    "data_training_NR = data_training.loc[data_training.Classificacao == 0]\n",
    "TrainingString_Relevant = ''\n",
    "TrainingString_NotRelevant = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, podemos visualizar o formato que o dataframe se encontra. Ele possu√≠ 2 colunas (fora o √≠ndice), uma delas indica qual a classifica√ß√£o (1 √© relevante, e 0 √© irrelevante), e a outra mostra o tweet que foi avaliado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Classificacao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uma corrida com esse uber a √∫nica forma de pag...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uber testa em s√£o paulo programa que avalia qu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vcs que andam de uber nem falem comigo, al√©m d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a fic: terminei o √∫ltimo epis√≥dio de glee (dnv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>olivares, cida \\nsobre seu uber \\n\\nmas podia ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nossa que vontade de sentar no uber, quer dize...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adoro ler os feedbacks do uber https://t.co/lp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>uber com balinha tudo pra mim</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>so uber e fa√ßo isso todo dia... https://t.co/e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mo√ßo do uber: vc √© a melhor da classe? \\neu: n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento  Classificacao\n",
       "0  uma corrida com esse uber a √∫nica forma de pag...              0\n",
       "1  uber testa em s√£o paulo programa que avalia qu...              1\n",
       "2  vcs que andam de uber nem falem comigo, al√©m d...              0\n",
       "3  a fic: terminei o √∫ltimo epis√≥dio de glee (dnv...              1\n",
       "4  olivares, cida \\nsobre seu uber \\n\\nmas podia ...              0\n",
       "5  nossa que vontade de sentar no uber, quer dize...              0\n",
       "6  adoro ler os feedbacks do uber https://t.co/lp...              1\n",
       "7                      uber com balinha tudo pra mim              1\n",
       "8  so uber e fa√ßo isso todo dia... https://t.co/e...              1\n",
       "9  mo√ßo do uber: vc √© a melhor da classe? \\neu: n...              1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_training.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos perceber, contudo, que esses tweets possuem pontua√ß√£o, emojis, e alguns caract√©res como **\\n**, que ir√£o poluir a an√°lise.\n",
    "\n",
    "Por isso, produzimos algumas fun√ß√µes que s√£o capazes de limpar os tweets, para facilitar a an√°lise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_laughter(word):\n",
    "    for letter in word:\n",
    "        if letter != 'k':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def cleanup(text):\n",
    "\n",
    "    text_split_emoji = emoji.get_emoji_regexp().split(text)\n",
    "    text_split_whitespace = [substr.split() for substr in text_split_emoji]\n",
    "    text_split = functools.reduce(operator.concat, text_split_whitespace)\n",
    "    text_split = ' '.join(word for word in text_split)\n",
    "    \n",
    "    text = ' '.join(word for word in text_split.split() if not word.startswith('https'))\n",
    "    \n",
    "    \n",
    "    punctuation = '[!-.:?;]'\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    text_subbed = text_subbed.lower()\n",
    "    text_out = ' '\n",
    "    \n",
    "    \n",
    "    for word in text_subbed.split():\n",
    "        if check_laughter(word):\n",
    "            text_out += ' haha'\n",
    "        else:\n",
    "            text_out += ' ' + word\n",
    "    \n",
    "    \n",
    "    return text_out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  x gon give it to ya fuck wait for you to get it on your own x gon deliver to ya knock knock open up the door it s real wit the non stop pop pop and stainless steel go hard gettin busy wit it uber blah blah haha üë®üèø üò∑ üò∑ uber blah blah haha üë®üèø üò∑ üò∑\n",
      "['x', 'gon', 'give', 'it', 'to', 'ya', 'fuck', 'wait', 'for', 'you', 'to', 'get', 'it', 'on', 'your', 'own', 'x', 'gon', 'deliver', 'to', 'ya', 'knock', 'knock', 'open', 'up', 'the', 'door', 'it', 's', 'real', 'wit', 'the', 'non', 'stop', 'pop', 'pop', 'and', 'stainless', 'steel', 'go', 'hard', 'gettin', 'busy', 'wit', 'it', 'uber', 'blah', 'blah', 'haha', 'üë®üèø', 'üò∑', 'üò∑', 'uber', 'blah', 'blah', 'haha', 'üë®üèø', 'üò∑', 'üò∑']\n"
     ]
    }
   ],
   "source": [
    "### --- Cleanup function demo --- ###\n",
    "demo_string_ =  \"\"\"\n",
    "                X gon give it to ya\n",
    "                Fuck wait for you to get it on your own\n",
    "                X gon deliver to ya\n",
    "                Knock knock, open up the door, it's real\n",
    "                Wit the non-stop, pop pop and stainless steel\n",
    "                Go hard gettin busy wit it\n",
    "                https://img.imgur/1337, uber blah blah kkk, üë®üèøüò∑üò∑\n",
    "                uber blah blah kkkkkkk https://img.imgur/1337, üë®üèøüò∑üò∑\n",
    "                \"\"\"\n",
    "\n",
    "print(cleanup(demo_string_))\n",
    "print(cleanup(demo_string_).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acima, temos um exemplo de uso da fun√ß√£o que limpa os textos necess√°rios. Ela √© capaz de remover **\\n** (caract√©res de quebra de linha), hyperlinks (haviam v√°rios, que n√£o significavam nada), pontua√ß√µes, e tamb√©m separa emojis das palavras e entre si. Outra coisa que ser√° √∫til para a an√°lise √© transformar risadas (comumente representadas por uma s√©rie de caract√©res *k* seguidos), em uma representa√ß√£o padr√£o, que foi escolhida como 'haha'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phrase in data_training_R.Treinamento:\n",
    "    TrainingString_Relevant += phrase\n",
    "for phrase in data_training_NR.Treinamento:\n",
    "    TrainingString_NotRelevant += phrase    \n",
    "    \n",
    "cleanup(TrainingString_NotRelevant)\n",
    "cleanup(TrainingString_Relevant)\n",
    "\n",
    "TrainingSeries_Relevant = pd.Series(TrainingString_Relevant.split()).value_counts(True)\n",
    "TrainingSeries_NotRelevant = pd.Series(TrainingString_NotRelevant.split()).value_counts(True)\n",
    "\n",
    "\n",
    "TSR_Absolute = pd.Series(TrainingString_Relevant.split()).value_counts()\n",
    "TSNR_Absolute = pd.Series(TrainingString_NotRelevant.split()).value_counts()\n",
    "\n",
    "SampleSize_Relevant = len(TSR_Absolute)\n",
    "SampleSize_NotRelevant = len(TSNR_Absolute)\n",
    "SampleSize_Total = SampleSize_Relevant+SampleSize_NotRelevant\n",
    "\n",
    "TrainingSeries_Join = pd.Series((TrainingString_Relevant+TrainingString_NotRelevant).split()).value_counts(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando o Classificador Naive-Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_Relevant = SampleSize_Relevant/SampleSize_Total\n",
    "P_NotRelevant = SampleSize_NotRelevant/SampleSize_Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Classificador:\n",
    "\n",
    "def evaluate_relevance(tweet = \"\", alpha = alpha, V = V):\n",
    "    \n",
    "    ## -- Cleaning tweet\n",
    "    text = cleanup(tweet)\n",
    "    \n",
    "    ## -- Evaluates likelihood\n",
    "    sumR = 0\n",
    "    sumNR = 0\n",
    "    T_r = (SampleSize_Relevant+alpha*V)\n",
    "    T_nr = (SampleSize_NotRelevant+alpha*V)\n",
    "    \n",
    "    for word in text.split():\n",
    "        \n",
    "        if word in TrainingSeries_Relevant: \n",
    "            sumR += np.log((TSR_Absolute[word]+alpha)/T_r)\n",
    "        else:\n",
    "            sumR += np.log(alpha/T_r)\n",
    "        if word in TrainingSeries_NotRelevant: \n",
    "            sumNR += np.log((TSNR_Absolute[word]+alpha)/T_nr)\n",
    "        else:\n",
    "            sumNR += np.log(alpha/T_nr)\n",
    "    \n",
    "    return sumR>sumNR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance\n",
    "\n",
    "Agora, usamos a fun√ß√£o evaluate_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percebe-se ent√£o, que, da nossa valida√ß√£o 52.26% s√£o relevantes e 47.74% n√£o, de acordo com nossa avalia√ß√£o manual.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'True-True': 0.41708542713567837,\n",
       " 'True-False': 0.10552763819095477,\n",
       " 'False-True': 0.36180904522613067,\n",
       " 'False-False': 0.11557788944723618,\n",
       " 'Final-Accuracy': '53.266%'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations = [int(evaluate_relevance(tweet, 1, 1250)) for tweet in data_validation.Teste]\n",
    "\n",
    "data_validation['bayes'] = evaluations\n",
    "\n",
    "Accuracy = {'True-True':0,\n",
    "        'True-False':0,\n",
    "        'False-True':0,\n",
    "        'False-False':0\n",
    "       }\n",
    "\n",
    "\n",
    "for tweet,evaluation in zip(data_validation.classificacao, data_validation.bayes):\n",
    "    if tweet and evaluation:\n",
    "        Accuracy['True-True'] += 1\n",
    "    elif tweet and not evaluation:\n",
    "        Accuracy['True-False'] += 1\n",
    "    elif not tweet and evaluation:\n",
    "        Accuracy['False-True'] += 1\n",
    "    elif not tweet and not evaluation:\n",
    "        Accuracy['False-False'] += 1\n",
    "\n",
    "Accuracy_Normalized = {}\n",
    "S = sum(Accuracy.values())\n",
    "for k,v in Accuracy.items():\n",
    "    Accuracy_Normalized[k] = v/S\n",
    "Accuracy_Normalized['Final-Accuracy'] = str(round((Accuracy_Normalized['True-True']+Accuracy_Normalized['False-False'])*100, 3)) + '%'\n",
    "lista.append(Accuracy_Normalized['Final-Accuracy'])\n",
    "a = data_validation.classificacao.value_counts(True)\n",
    "print(\"Percebe-se ent√£o, que, da nossa valida√ß√£o {1}% s√£o relevantes e {0}% n√£o, de acordo com nossa avalia√ß√£o manual.\".format(round(a[0]*100,2),round(a[1]*100, 2)))\n",
    "Accuracy_Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A acur√°cia ideal seria: 58.794%'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Usando SKLearn:\n",
    "\n",
    "S = data_training\n",
    "S_teste = data_validation\n",
    "\n",
    "Xt_train = S['Treinamento']\n",
    "y_train = S['Classificacao'] == 1\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "count = CountVectorizer()\n",
    "X_train = count.fit_transform(Xt_train)\n",
    "\n",
    "model = MultinomialNB(alpha=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "Xt_test = S_teste['Teste']\n",
    "y_test = S_teste['classificacao'] == 1\n",
    "\n",
    "X_test = count.transform(Xt_test)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\"A acur√°cia ideal seria: {0}%\".format(round(accuracy_score(y_test, y_pred)*100,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo\n",
    "\n",
    "### Reflex√£o sobre os resultados\n",
    "\n",
    "A partir da an√°lise dos dados, podemos concluir que nosso classificador Na√Øve-Bayes √© p√©ssimo, pois ele s√≥ √© melhor do que presumir que tudo √© relevante, por 1%. Alguns motivos para explicar por que isso ocorreu seriam; A baixa quantidade de tweets de treinamento (400tweets), a alta subjetividade do tema (dif√≠cil dizer se um tweet √© positivo ou negativo, muitas vezes por sarcasmo, por exemplo), ou ainda, porque n√£o separamos palavras em grupos sem√¢nticos (como flex√µes de um mesmo verbo).\n",
    "\n",
    "___\n",
    "\n",
    "### Outros usos do classificador Na√Øve-Bayes\n",
    "\n",
    "Esse tipo de classificador tem v√°rios usos. Muitos fen√¥menos podem ser previsto com enorme acur√°cia, [como por exemplo detec√ß√£o de spam, categoriza√ß√£o de not√≠cias, reconhecimento facial em fotos, diagn√≥stico m√©dico, reconhecimento de d√≠gitos, precis√£o do tempo, (essencialmente qualquer cen√°rio, onde a hip√≥tese que os eventos s√£o independentes entre si seja razo√°vel)](https://www.quora.com/In-what-real-world-applications-is-Naive-Bayes-classifier-used) etc.\n",
    "\n",
    "___\n",
    "\n",
    "### Por qu√™ n√£o treinar o modelo com ele mesmo, ou outro modelo automatizado\n",
    "\n",
    "N√£o faria sentido treinar o modelo com dados que o pr√≥prio modelo produziu, j√° que assim, ele s√≥ iria fixar v√≠eses e erros que j√° cometia antes. Trein√°-lo com outro modelo seria poss√≠vel, por√©m n√£o seria t√£o simples, e envolveria conhecimentos sobre outras t√©cnicas de machine learning, como aprendizado adversarial ou cooperativo. \n",
    "\n",
    "___\n",
    "\n",
    "### Sobre o tratamento de palavras\n",
    "\n",
    "Existem m√∫ltiplas t√©cnicas de limpar textos, como o *stemming*, que poderiam ajudar muito a an√°lise. Neste projeto, apenas removemos carac√©teres como **\\n**, que n√£o possuem significado, assim como *hyperlinks* desnecess√°rios (como ***https:/t.co.11613613.com***), e tamb√©m fazemos algo parecido com o stemming para risadas, transformando qualquer string composta somente por ***k***'s, em ***haha***. Tamb√©m separamos adequadamente os emojis, permitindo que sejam analisados como elementos individuais.\n",
    "\n",
    "\n",
    "\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperfei√ßoamento:\n",
    "\n",
    "Os trabalhos v√£o evoluir em conceito dependendo da quantidade de itens avan√ßados:\n",
    "\n",
    "* Limpar: \\n, :, \", ', (, ), etc SEM remover emojis [OK]()\n",
    "* Corrigir separa√ß√£o de espa√ßos entre palavras e emojis ou emojis e emojis [OK]()\n",
    "* Propor outras limpezas e transforma√ß√µes que n√£o afetem a qualidade da informa√ß√£o ou classifica√ß√£o [OK]()\n",
    "* Criar categorias intermedi√°rias de relev√¢ncia baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante (3 categorias: C, mais categorias conta para B)\n",
    "* Explicar por que n√£o posso usar o pr√≥prio classificador para gerar mais amostras de treinamento [OK]()\n",
    "* Propor diferentes cen√°rios para Na√Øve Bayes fora do contexto do projeto [OK]()\n",
    "* Sugerir e explicar melhorias reais com indica√ß√µes concretas de como implementar (indicar como fazer e indicar material de pesquisa) [OK]()\n",
    "* Montar um dashboard que periodicamente realiza an√°lise de sentimento e visualiza estes dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refer√™ncias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**\n",
    "\n",
    "[Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) **Explica√ß√£o da t√©cnica usada para lidar com palavras novas**\n",
    "\n",
    "[Other applications of Na√Øve-Bayes](https://www.quora.com/In-what-real-world-applications-is-Naive-Bayes-classifier-used) **Exemplos de uso do classificador em outros contextos**\n",
    "\n",
    "[Explanation fo beta-smoothing and calculating the alpha and beta coefficients](https://stats.stackexchange.com/questions/47916/bayesian-batting-average-prior/47921#47921) **Outro smoothing, que n√£o foi utilizado**\n",
    "\n",
    "[More about beta smoothing](https://stats.stackexchange.com/questions/47771/what-is-the-intuition-behind-beta-distribution/47782#47782) **Mais detalhes sobre o beta smoothing**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
