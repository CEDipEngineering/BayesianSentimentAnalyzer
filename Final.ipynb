{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Ci√™ncia dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Carlos Eduardo Dip\n",
    "\n",
    "Nome: Gianluca Lazzaris Giudici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposta\n",
    "___\n",
    "\n",
    "Neste projeto, a proposta √© criar um identificador de sentimento, a partir de um analisador Na√Øve-Bayes, que faz uso principalmente do Teorema de Bayes, e √© um modelo de Machine Learning.\n",
    "___\n",
    "# Classificador autom√°tico de sentimento\n",
    "\n",
    "<img src = 'logo.jpg' style = 'width:30%'>\n",
    "\n",
    "Empresa escolhida: Uber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Preparando o ambiente no jupyter:\n",
    "\n",
    "Primeiro importamos os m√≥dulos que ser√£o utilizados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import functools\n",
    "import operator\n",
    "from random import shuffle\n",
    "\n",
    "try:\n",
    "    import emoji\n",
    "except:\n",
    "    !pip install emoji --upgrade\n",
    "    import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Coleta de Dados:\n",
    "\n",
    "Para a an√°lise que ser√° feita, ser√£o usados tweets (da plataforma Twitter), que ser√£o extra√≠dos atrav√©s de uma API chamada Tweepy;\n",
    "Os dados foram coletados em outro notebook, chamado Coletor.ipynb. Abaixo, ser√° importado o excel com os dados j√° classificados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Limpeza, aquisi√ß√£o e organiza√ß√£o dos dados\n",
    "\n",
    "Primeiro, constru√≠mos dataframes (da biblioteca Pandas), para facilitar a visualiza√ß√£o e utiliza√ß√£o dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training = pd.read_excel('data/Uber.xlsx')\n",
    "data_validation = pd.read_excel('data/Uber.xlsx', 'Teste')\n",
    "\n",
    "data_training_R = data_training.loc[data_training.Classificacao == 1]\n",
    "data_training_NR = data_training.loc[data_training.Classificacao == 0]\n",
    "TrainingString_Relevant = ''\n",
    "TrainingString_NotRelevant = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, podemos visualizar o formato que o dataframe se encontra. Ele possu√≠ 2 colunas (fora o √≠ndice), uma delas indica qual a classifica√ß√£o (1 √© relevante, e 0 √© irrelevante), e a outra mostra o tweet que foi avaliado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Classificacao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uma corrida com esse uber a √∫nica forma de pag...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uber testa em s√£o paulo programa que avalia qu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vcs que andam de uber nem falem comigo, al√©m d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a fic: terminei o √∫ltimo epis√≥dio de glee (dnv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>olivares, cida \\nsobre seu uber \\n\\nmas podia ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nossa que vontade de sentar no uber, quer dize...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adoro ler os feedbacks do uber https://t.co/lp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>uber com balinha tudo pra mim</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>so uber e fa√ßo isso todo dia... https://t.co/e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mo√ßo do uber: vc √© a melhor da classe? \\neu: n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento  Classificacao\n",
       "0  uma corrida com esse uber a √∫nica forma de pag...              0\n",
       "1  uber testa em s√£o paulo programa que avalia qu...              1\n",
       "2  vcs que andam de uber nem falem comigo, al√©m d...              0\n",
       "3  a fic: terminei o √∫ltimo epis√≥dio de glee (dnv...              1\n",
       "4  olivares, cida \\nsobre seu uber \\n\\nmas podia ...              0\n",
       "5  nossa que vontade de sentar no uber, quer dize...              0\n",
       "6  adoro ler os feedbacks do uber https://t.co/lp...              1\n",
       "7                      uber com balinha tudo pra mim              1\n",
       "8  so uber e fa√ßo isso todo dia... https://t.co/e...              1\n",
       "9  mo√ßo do uber: vc √© a melhor da classe? \\neu: n...              1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_training.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.56\n",
       "0    0.44\n",
       "Name: Classificacao, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_training.Classificacao.value_counts(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a s√©rie acima, percebemos que 56% dos tweets s√£o relevantes, e 44% n√£o, de acordo com nossa avalia√ß√£o manual.\n",
    "\n",
    "Podemos perceber, contudo, que esses tweets possuem pontua√ß√£o, emojis, e alguns caract√©res como **\\n**, que ir√£o poluir a an√°lise.\n",
    "\n",
    "Por isso, produzimos algumas fun√ß√µes que s√£o capazes de limpar os tweets, para facilitar a an√°lise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_laughter(word):\n",
    "    for letter in word:\n",
    "        if letter != 'k':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def cleanup(text):\n",
    "\n",
    "    text_split_emoji = emoji.get_emoji_regexp().split(text)\n",
    "    text_split_whitespace = [substr.split() for substr in text_split_emoji]\n",
    "    text_split = functools.reduce(operator.concat, text_split_whitespace)\n",
    "    text_split = ' '.join(word for word in text_split)\n",
    "    \n",
    "    text = ' '.join(word for word in text_split.split() if not word.startswith('https'))\n",
    "    \n",
    "    \n",
    "    punctuation = '[!-.:?;]'\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    text_subbed = text_subbed.lower()\n",
    "    text_out = ' '\n",
    "    \n",
    "    \n",
    "    for word in text_subbed.split():\n",
    "        if check_laughter(word):\n",
    "            text_out += ' haha'\n",
    "        else:\n",
    "            text_out += ' ' + word\n",
    "    \n",
    "    \n",
    "    return text_out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  x gon give it to ya fuck wait for you to get it on your own x gon deliver to ya knock knock open up the door it s real wit the non stop pop pop and stainless steel go hard gettin busy wit it uber blah blah haha üë®üèø üò∑ üò∑ uber blah blah haha üë®üèø üò∑ üò∑\n",
      "['x', 'gon', 'give', 'it', 'to', 'ya', 'fuck', 'wait', 'for', 'you', 'to', 'get', 'it', 'on', 'your', 'own', 'x', 'gon', 'deliver', 'to', 'ya', 'knock', 'knock', 'open', 'up', 'the', 'door', 'it', 's', 'real', 'wit', 'the', 'non', 'stop', 'pop', 'pop', 'and', 'stainless', 'steel', 'go', 'hard', 'gettin', 'busy', 'wit', 'it', 'uber', 'blah', 'blah', 'haha', 'üë®üèø', 'üò∑', 'üò∑', 'uber', 'blah', 'blah', 'haha', 'üë®üèø', 'üò∑', 'üò∑']\n"
     ]
    }
   ],
   "source": [
    "### --- Cleanup function demo --- ###\n",
    "demo_string_ =  \"\"\"\n",
    "                X gon give it to ya\n",
    "                Fuck wait for you to get it on your own\n",
    "                X gon deliver to ya\n",
    "                Knock knock, open up the door, it's real\n",
    "                Wit the non-stop, pop pop and stainless steel\n",
    "                Go hard gettin busy wit it\n",
    "                https://img.imgur/1337, uber blah blah kkk, üë®üèøüò∑üò∑\n",
    "                https://img.imgur/1337, uber blah blah kkkkkkk, üë®üèøüò∑üò∑\n",
    "                \"\"\"\n",
    "\n",
    "print(cleanup(demo_string_))\n",
    "print(cleanup(demo_string_).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acima, temos um exemplo de uso da fun√ß√£o que limpa os textos necess√°rios. Ela √© capaz de remover **\\n** (caract√©res de quebra de linha), hyperlinks, pontua√ß√µes, e tamb√©m separa emojis das palavras e entre si. Outra coisa que ser√° √∫til para a an√°lise √© transformar risadas (comumente representadas por uma s√©rie de caract√©res *k* seguidos), em uma representa√ß√£o padr√£o, que foi escolhida como 'haha'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uber            0.047110\n",
      "o               0.030094\n",
      "de              0.028688\n",
      "e               0.027282\n",
      "que             0.021938\n",
      "eu              0.018563\n",
      "do              0.017719\n",
      "a               0.016313\n",
      "pra             0.015750\n",
      "no              0.014906\n",
      "n√£o             0.013219\n",
      "um              0.011813\n",
      "com             0.010125\n",
      "me              0.009844\n",
      "q               0.009281\n",
      "em              0.007313\n",
      "da              0.007313\n",
      "na              0.007031\n",
      "√©               0.006469\n",
      "mais            0.006188\n",
      "t√°              0.006188\n",
      "uma             0.006188\n",
      "minha           0.005906\n",
      "s√≥              0.005344\n",
      "meu             0.005344\n",
      "por             0.005063\n",
      "ele             0.005063\n",
      "tem             0.005063\n",
      "ir              0.004219\n",
      "motorista       0.004219\n",
      "                  ...   \n",
      "lugar           0.000281\n",
      "desesperada     0.000281\n",
      "emprego         0.000281\n",
      "vejo            0.000281\n",
      "carro.          0.000281\n",
      "ouvindo         0.000281\n",
      "descarada       0.000281\n",
      "vida!           0.000281\n",
      "fosse           0.000281\n",
      "ado√ß√£o          0.000281\n",
      "kkkkkkkkkkkk    0.000281\n",
      "chora           0.000281\n",
      "@talgarrao      0.000281\n",
      "trem            0.000281\n",
      "problemas       0.000281\n",
      "fa√ßa            0.000281\n",
      "lata            0.000281\n",
      "m√£os            0.000281\n",
      "atrasar         0.000281\n",
      "feijoada        0.000281\n",
      "fone,           0.000281\n",
      "r√°pido.         0.000281\n",
      "restaurante     0.000281\n",
      "contando        0.000281\n",
      "arabe           0.000281\n",
      "no√ß√£o           0.000281\n",
      "c√©usquando      0.000281\n",
      "for             0.000281\n",
      "nervosouber     0.000141\n",
      "nervoso         0.000141\n",
      "Length: 1422, dtype: float64\n",
      "uber                        0.046883\n",
      "de                          0.031611\n",
      "o                           0.026638\n",
      "e                           0.021311\n",
      "que                         0.019890\n",
      "do                          0.019535\n",
      "eu                          0.018469\n",
      "a                           0.017759\n",
      "um                          0.017048\n",
      "pra                         0.014562\n",
      "√©                           0.012431\n",
      "no                          0.011366\n",
      "n√£o                         0.009590\n",
      "com                         0.009235\n",
      "me                          0.008169\n",
      "uma                         0.007636\n",
      "ele                         0.007104\n",
      "se                          0.006393\n",
      "vai                         0.006038\n",
      "q                           0.006038\n",
      "na                          0.005683\n",
      "da                          0.005328\n",
      "meu                         0.004972\n",
      "em                          0.004617\n",
      "minha                       0.004262\n",
      "s√≥                          0.004262\n",
      "esse                        0.004262\n",
      "cara                        0.003907\n",
      "vou                         0.003907\n",
      "pro                         0.003907\n",
      "                              ...   \n",
      "faz                         0.000355\n",
      "porrase                     0.000355\n",
      "casa                        0.000355\n",
      "maior                       0.000355\n",
      "endere√ßo                    0.000355\n",
      "lugar                       0.000355\n",
      "https://t.co/4ozmoysqi3o    0.000355\n",
      "devolver                    0.000355\n",
      "tbm                         0.000355\n",
      "pr√≥ximo                     0.000355\n",
      "falou:                      0.000355\n",
      "quando                      0.000355\n",
      "ifb                         0.000355\n",
      "acabei                      0.000355\n",
      "poolüëçüèª                      0.000355\n",
      "galeria                     0.000355\n",
      "eita                        0.000355\n",
      "uber?40                     0.000355\n",
      "augeo                       0.000355\n",
      "entendi.                    0.000355\n",
      "dados                       0.000355\n",
      "mentir,                     0.000355\n",
      "federal                     0.000355\n",
      "exigimos                    0.000355\n",
      "artistas...                 0.000355\n",
      "usa                         0.000355\n",
      "deixando                    0.000355\n",
      "whatsapp                    0.000355\n",
      "doidauma                    0.000178\n",
      "doida                       0.000178\n",
      "Length: 1281, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for phrase in data_training_R.Treinamento:\n",
    "    TrainingString_Relevant += phrase\n",
    "for phrase in data_training_NR.Treinamento:\n",
    "    TrainingString_NotRelevant += phrase    \n",
    "    \n",
    "cleanup(TrainingString_NotRelevant)\n",
    "cleanup(TrainingString_Relevant)\n",
    "\n",
    "TrainingSeries_Relevant = pd.Series(TrainingString_Relevant.split()).value_counts(True)\n",
    "TrainingSeries_NotRelevant = pd.Series(TrainingString_NotRelevant.split()).value_counts(True)\n",
    "\n",
    "SampleSize_Relevant = len(TrainingString_Relevant.split())\n",
    "SampleSize_NotRelevant = len(TrainingString_NotRelevant.split())\n",
    "SampleSize_Total = SampleSize_Relevant+SampleSize_NotRelevant\n",
    "\n",
    "TrainingSeries_Join = pd.Series((TrainingString_Relevant+TrainingString_NotRelevant).split()).value_counts(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando o Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_Relevant = SampleSize_Relevant/SampleSize_Total\n",
    "P_NotRelevant = SampleSize_NotRelevant/SampleSize_Total\n",
    "\n",
    "media, stdev = TrainingSeries_Join.mean(), TrainingSeries_Join.std()\n",
    "\n",
    "alpha = ((media*(1-media))/stdev**2 -1)*media\n",
    "beta = ((media*(1-media))/stdev**2 -1)*(1-media)\n",
    "\n",
    "\n",
    "# print(alpha, beta, media, stdev)\n",
    "# print(alpha/(alpha+beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_relevance(tweet = \"\"):\n",
    "    \n",
    "    ## -- Cleaning tweet\n",
    "    text = cleanup(tweet)\n",
    "    \n",
    "    ## -- Evaluates likelihood\n",
    "    sumR = 0\n",
    "    sumNR = 0\n",
    "    \n",
    "    for word in text.split():\n",
    "    \n",
    "        if word in TrainingSeries_Relevant: \n",
    "            sumR += np.log((TrainingSeries_Relevant[word]+alpha))\n",
    "        else:\n",
    "            sumR += np.log(alpha/(alpha+beta))\n",
    "        if word in TrainingSeries_NotRelevant: \n",
    "            sumNR += np.log((TrainingSeries_NotRelevant[word]+alpha))\n",
    "        else:\n",
    "            sumNR += np.log(alpha/(alpha+beta))\n",
    "     \n",
    "    \n",
    "    return sumR>sumNR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance\n",
    "\n",
    "Agora voc√™ deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = [int(evaluate_relevance(tweet)) for tweet in data_validation.Teste]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_validation['bayes'] = evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'True-True': 0.2964824120603015,\n",
       " 'True-False': 0.22613065326633167,\n",
       " 'False-True': 0.2562814070351759,\n",
       " 'False-False': 0.22110552763819097,\n",
       " 'Final-Accuracy': '51.759%'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy = {'True-True':0,\n",
    "            'True-False':0,\n",
    "            'False-True':0,\n",
    "            'False-False':0\n",
    "           }\n",
    "\n",
    "\n",
    "for tweet,evaluation in zip(data_validation.classificacao, data_validation.bayes):\n",
    "    if tweet and evaluation:\n",
    "        Accuracy['True-True'] += 1\n",
    "    elif tweet and not evaluation:\n",
    "        Accuracy['True-False'] += 1\n",
    "    elif not tweet and evaluation:\n",
    "        Accuracy['False-True'] += 1\n",
    "    elif not tweet and not evaluation:\n",
    "        Accuracy['False-False'] += 1\n",
    "\n",
    "Accuracy_Normalized = {}\n",
    "S = sum(Accuracy.values())\n",
    "for k,v in Accuracy.items():\n",
    "    Accuracy_Normalized[k] = v/S\n",
    "Accuracy_Normalized['Final-Accuracy'] = str(round((Accuracy_Normalized['True-True']+Accuracy_Normalized['False-False'])*100, 3)) + '%'\n",
    "Accuracy_Normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperfei√ßoamento:\n",
    "\n",
    "Os trabalhos v√£o evoluir em conceito dependendo da quantidade de itens avan√ßados:\n",
    "\n",
    "* Limpar: \\n, :, \", ', (, ), etc SEM remover emojis\n",
    "* Corrigir separa√ß√£o de espa√ßos entre palavras e emojis ou emojis e emojis\n",
    "* Propor outras limpezas e transforma√ß√µes que n√£o afetem a qualidade da informa√ß√£o ou classifica√ß√£o\n",
    "* Criar categorias intermedi√°rias de relev√¢ncia baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante (3 categorias: C, mais categorias conta para B)\n",
    "* Explicar por que n√£o posso usar o pr√≥prio classificador para gerar mais amostras de treinamento\n",
    "* Propor diferentes cen√°rios para Na√Øve Bayes fora do contexto do projeto\n",
    "* Sugerir e explicar melhorias reais com indica√ß√µes concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* Montar um dashboard que periodicamente realiza an√°lise de sentimento e visualiza estes dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refer√™ncias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**\n",
    "\n",
    "[Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) **Explica√ß√£o da t√©cnica para lidar com palavras novas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
